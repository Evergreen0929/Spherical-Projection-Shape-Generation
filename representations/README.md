# Data Preprocessing Pipeline

This document outlines the steps to preprocess raw 3D models into various data representations (omnidirectional maps, single-view images, and latents) required for training.

## Environment Setup

Install the main dependencies using `pip`. The tested versions are listed below.

```bash
pip install numpy==1.23.5 trimesh==3.23.5 opencv-python==4.6.0.66 open3d==0.16.0
pip install tqdm objaverse
```

## 1. Download Data (`download_data.py`)

* **Script:** `download_data.py`
* **Purpose:** To download the raw 3D models from the Objaverse dataset based on predefined training and testing splits.
* **How it works:**
    * It first loads lists of training and testing object UIDs from a local `.npz` file (`uid_dict.npz`).
    * It then uses the `objaverse.load_objects` function to download the specified objects.
    * The download process is parallelized using `multiprocessing.cpu_count()` to improve speed.

* **Expected Data Structure (Input for next steps):**
    The subsequent scripts (like `prepare_sp_depth_map.py`) expect the downloaded `.glb` or `.obj` files to be organized into a source directory, typically structured by subfolders:

    ```
    <dataset_root>/
    └── glbs/
        ├── <subdir_1>/       (e.g., '000-000')
        │   ├── <uid_1>.glb
        │   └── <uid_2>.glb
        └── <subdir_2>/
            ├── <uid_3>.glb
            ...
    ```

## 2. Generate Spherical Projected Depth Maps (`prepare_sp_depth_map.py`)

* **Script:** `prepare_sp_depth_map.py`
* **Purpose:** To convert the 3D models into spherical depth maps (also referred to as OS maps in code).
* **How it works:**
    * The script iterates through the `glbs/` directory.
    * For each model, it loads the mesh using `trimesh` and normalizes it by centering it and scaling it to fit within a unit cube.
    * A ground-truth version of the normalized mesh is saved as `mesh.ply` in the destination folder.
    * It then calls `fast_project_mesh_to_sphere` to perform spherical projection. This function casts rays from a sphere's surface inward to calculate the distance to the mesh surface.
    * It generates multiple depth layers (up to `max_depth`) to handle occlusions and complex geometry.
    * These depth maps are saved as 16-bit PNG files (e.g., `depth_0.png`, `depth_1.png`) in a `maps/` subfolder.

* **Output Location (for one object):**
    ```
    <dataset_root>/
    └── preprocessed/
        └── <subdir>/
            └── <uid>/
                ├── mesh.ply
                └── maps/
                    ├── depth_0.png
                    ├── depth_1.png
                    ...
    ```

## 3. Quality Analysis & Filtering (`quality_analysis.py`, optional)

* **Script:** `quality_analysis.py`
* **Purpose:** To validate the quality of generated maps by reconstructing a 3D mesh from them and comparing it to the ground-truth mesh. This script filters out low-quality samples.
* **Note:** This script expects **Position (`pos_*.png`)** and **Normal (`normal_*.png`)** maps, which are assumed to be generated by a separate process (e.g., a VAE or other map generator) and stored in the `maps/` folder.
* **How it works:**
    * It iterates through the `preprocessed` directory, loading the `pos` and `normal` maps for each sample.
    * These maps are converted into a 3D point cloud (`selected_pos`, `selected_normal`).
    * It uses Open3D's Poisson surface reconstruction (`create_from_point_cloud_poisson`) to create a new mesh from this point cloud.
    * It loads the ground-truth `mesh.ply` (generated in Step 2).
    * It computes the **Chamfer distance** between the reconstructed mesh and the ground-truth mesh.
    * If the Chamfer distance is too high (`> 0.05`), the entire sample directory is moved to a `discarded` folder.
    * It also performs a check (`validation_check`) on the angle between position vectors and normals to find invalid normals.
    * Finally, it reports the mean Chamfer distance for all samples that pass the quality check.

## 4. Render Single-View Images (`render_codes/`)

* **Scripts:** `distributed.py`, `render_batch_persp.sh`, `blenderProc_persp.py`
* **Purpose:** To render 2D single-view images (e.g., RGB, normals) from multiple camera angles using Blender.
* **Environment Setup:**
    * This rendering pipeline is based on `BlenderProc`.
    * To install dependencies, navigate to the `render_codes/` directory and run:
        ```bash
        cd ./render_codes
        pip install -r requirements.txt
        ```

* **How it works (Batch Mode):**
    * The batch rendering process is launched using a shell script, such as `render_batch_persp.sh`.
    * This script executes `distributed.py`, a multiprocessing script that distributes the rendering workload across multiple GPUs.
    * `distributed.py` reads a JSON file (`--input_models_path`) containing the list of all models to be rendered.
    * Each worker process calls a BlenderProc script (e.g., `blenderProc_persp.py` for perspective views) for a single object.
    * The `blenderProc_persp.py` script loads the object in Blender, normalizes its scale and position, and sets up 9 cameras at fixed locations (front, back, left, right, top, and corners).
    * It then renders and saves the RGB image (as `.webp`), normal map (as `.webp`), and camera parameters (K and RT matrices as `.txt`) for each of the 9 views.
    * We disabled the random rotation of objects during rendering.

* **Output Location:**
    The rendered images are saved to the `--save_folder` specified in the batch script (e.g., `/data/nineviews-pinhole`), following a structure defined in `distributed.py`:

    ```
    <save_folder>/
    └── <uid_prefix>/       (e.g., 'c7')
        └── <uid>/            (e.g., 'c70e8817...')
            ├── rgb_002_front.webp
            ├── normals_002_front.webp (not used)
            ├── 002_front_K.txt (not used)
            ├── 002_front_RT.txt (not used)
            ... (and for all other views)
    ```
    *(Inferred from `distributed.py`'s `view_path` logic and `blenderProc_persp.py`'s save logic)*

## 5. Generate SP Map Latents (via VAE)

* **Purpose:** This step involves using a pre-trained VAE (Variational Autoencoder) to encode the SP depth maps (generated in Step 2) into a compressed latent representation.
* **Status:** The code for this step has not yet been uploaded to the repository.

## 6. Final Validation Check (`validation_check.py`)

* **Script:** `validation_check.py`
* **Purpose:** To perform a final audit of the *fully processed* dataset to ensure that all required files for training are present and complete. This script assumes all previous steps (SP map generation, rendering, latent generation) have been completed and their outputs merged into a single `preprocessed` directory.
* **How it works:**
    * It iterates through all samples in the `preprocessed` directory.
    * For each sample, it checks for the existence and completeness of several key components:
        * `mesh.ply` (the ground-truth mesh).
        * `maps/` (checks for a specific number of files, e.g., 8).
        * `render_images/` (checks for a specific number of files, e.g., 14).
        * `latents/` (specifically, `map_latent_path`, checking for 8 files).
    * The script counts the total number of "successful" (complete) and "failed" (incomplete) data samples and prints a report, including the specific reason for any failures.